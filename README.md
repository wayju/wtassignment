# Project Title

Take home assignment for WT.

## Description

Console application for fetching IP addresses for hostnames for the purpose of detecting changes.

Output will include both hostnames which have changed and those which have not. Uses the google DoH api to fetch the IP Address from DNS A records. Results may not be returned in the same order as input. Multiple *A* records will be returned separated with a space.

## Getting Started

### Dependencies

* Docker if running with Docker, or otherwise, 
* Node.js >= v16 & Typescript installed globally.
* Bash (mac / linux) if using the helper script to upload/fetch files from the application in docker.

### Execution

#### Docker

Don't use the Dockerfile without compose as it is not currently setup in a useful way as there are input and output files involved.

#### Docker Compose

The docker compose setup outlined and configurations is for development using docker-compose. For development purposes the compose file will mount the project folder and run `npx tsc --watch` to watch for changes. The sample files will be included so it is possible to use these for execution.

1. `cd` to the project directory
2. `docker compose -f docker-compose.dev.yml up`
3. `docker compose -f docker-compose.dev.yml exec app rm -f output.csv && node dist/index.js -F samples/example.csv -B 10`
4. `docker compose -f docker-compose.dev.yml exec app cat output.csv`

In order to use a different sample file use the provided script to upload the input file, execute the application and retrieve the result file. Alternatively copy the commands from the script and modify appropriately if using windows. Adapted from script created by Chat GPT.

1. `cd` to the project directory
2. `docker compose -f docker-compose.dev.yml up`
3. Ensure that `run.sh` is executable
4. `./run.sh someinputfile.csv`

##### Node

It is of course possible to run the application directly once node and typescript are installed:

1 View the help command to see options `npm start -- -h`
2 Run the application `npm start -- -F samples/example.csv`
3 Run lint `npm run lint`
4 Run tests `npm test`

### Input and Output files

Input and output files share the same csv format with the exception that IP address is not required for input files. No header line is required. Empty lines in the input file will be skipped. For multiple `A` records the IP addresses are separated by a space ` `.

e.g. 

```www.twitter.com,104.244.42.1 104.244.42.65 104.244.42.193 104.244.42.129
www.netflix.com,44.234.232.238 44.237.234.25 44.242.60.85
www.nytimes.com,151.101.1.164 151.101.65.164 151.101.193.164 151.101.129.164
facebook.com,157.240.15.35
wikipedia.com,103.102.166.226
www.github.com,20.205.243.166
www.atlassian.com```


## Notes

* Arguably the command / option functionality is unnecessary for such a simple app but its quite neat and isnâ€™t very complex.
* Given that this is a small console application the source hierarchy is domain based rather than functional. It's almost simple enough to be flat structure but its easier to navigate given the additional files for unit tests.
* No code was generated by an LLM other than a script for running via docker with a file upload (annotated as such). I used Chat GPT to answer a few general purpose questions as we would normally use google / stack overflow. I also used Chat GPT to generate the sample files and a script as described.
* I choose Winston for logging. Primarily because I wanted something quick which could easily log simple formatted strings rather than JSON which I find easier for development. Most of the lightweight logging frameworks default to JSON.
* I did a very quick look to see whether the google API throttles or rate limits but I didnt find anything immediately. For actual usage this should be discovered and I suspect the TOS needs to be consulted.

## Potential Improvements

* A few unit tests are provided as an example. Coverage should be higher in a production application.
* I would like to spend a few hours improving the flow from input -> process -> output so that the file is buffered and processed at the same time rather than read into memory wholesale. The performance was acceptable for initial testing up to 1000 records and so I didn't see it as mandatory for this implementation. The slight complexity to this is to buffer, stream and batch at the same time so that the outgoing http requests are controlled.
* It Would be nice to support multiple separators. It feels more natural to use tab separation for this use case.

## Questions

Questions I would typically have asked product / business / stakeholders:

* How large would the input typically be? - to inform decisions around batching / perf testing / logging / reporting. Influences how we should perform the input processing mechanism.
* What will be done with the results? - this determines how best to build the input/output interface. For example do we want to return a count of failures / Hanging hostname, do we want to only include results for those which have changes or should we return all results. What should we output in the case of errors? It may even be prefered to use console input and output with flow managed outside of the process through infra (e.g. bash) scripts.
* What log format is desirable? What will process it.
* How should multiple IPs be formatted for multiple A records? At present they are space separated for simplicity however if a comparison was required for change detection then other formats magy be better.

